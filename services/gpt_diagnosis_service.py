import os
import json
import asyncio
import aiofiles
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple
from sqlalchemy import select, update

from database.models import Document, User, Application, DocumentType
from database.database import get_db_session
from services.document_service import DocumentService
from config.settings import get_settings
import logging

# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
try:
    import PyMuPDF as fitz  # pip install PyMuPDF
except ImportError:
    fitz = None

try:
    import openai  # pip install openai
except ImportError:
    openai = None

logger = logging.getLogger(__name__)

class GPTDiagnosisService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ GPT"""
    
    def __init__(self):
        self.settings = get_settings()
        self.document_service = DocumentService()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º OpenAI
        if openai:
            openai.api_key = getattr(self.settings, 'OPENAI_API_KEY', None)
    
    async def analyze_credit_history(
        self, 
        user_id: int, 
        application_id: Optional[int] = None
    ) -> Dict[str, Any]:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏"""
        
        try:
            logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ –ö–ò –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            
            # 1. –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            documents = await self._get_bki_documents(user_id, application_id)
            
            if not documents:
                return {
                    "success": False,
                    "error": "–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –ë–ö–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
                    "required_documents": ["–ù–ë–ö–ò", "–û–ö–ë", "–≠–∫–≤–∏—Ñ–∞–∫—Å"]
                }
            
            # 2. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–æ–≤
            extracted_texts = await self._extract_texts_from_documents(documents)
            
            if not extracted_texts:
                return {
                    "success": False,
                    "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
                }
            
            # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã –ë–ö–ò
            combined_text = await self._combine_bki_texts(extracted_texts)
            
            # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤ (120,000)
            if len(combined_text) > 120000:
                logger.warning(f"–¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                combined_text = combined_text[:120000] + "\n[–¢–ï–ö–°–¢ –û–ë–†–ï–ó–ê–ù]"
            
            # 5. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ GPT –Ω–∞ –∞–Ω–∞–ª–∏–∑
            gpt_result = await self._send_to_gpt(combined_text)
            
            if not gpt_result["success"]:
                return gpt_result
            
            # 6. –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç GPT
            analysis_result = await self._parse_gpt_response(gpt_result["response"])
            
            # 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            await self._save_analysis_result(user_id, application_id, analysis_result)
            
            logger.info(f"–ê–Ω–∞–ª–∏–∑ –ö–ò –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            
            return {
                "success": True,
                "analysis": analysis_result,
                "documents_analyzed": len(documents),
                "text_length": len(combined_text)
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ö–ò: {e}")
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"
            }
    
    async def _get_bki_documents(
        self, 
        user_id: int, 
        application_id: Optional[int] = None
    ) -> List[Document]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –ë–ö–ò –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        
        async with get_db_session() as session:
            query = select(Document).where(
                Document.user_id == user_id,
                Document.file_type.in_([
                    DocumentType.CREDIT_REPORT_NBKI,
                    DocumentType.CREDIT_REPORT_OKB,
                    DocumentType.CREDIT_REPORT_EQUIFAX
                ])
            )
            
            if application_id:
                query = query.where(Document.application_id == application_id)
            
            query = query.order_by(Document.uploaded_at.desc())
            result = await session.execute(query)
            
            documents = result.scalars().all()
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ë–ö–ò –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
            return documents
    
    async def _extract_texts_from_documents(
        self, 
        documents: List[Document]
    ) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        extracted_texts = {}
        
        for document in documents:
            try:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                file_data = await self.document_service.get_file_data(document)
                
                if not file_data:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {document.file_name}")
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                text = await self._extract_text_from_pdf(file_data, document.file_name)
                
                if text:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ë–ö–ò
                    bki_type = self._determine_bki_type(document.file_type, text)
                    extracted_texts[bki_type] = text
                    
                    logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç –∏–∑ {document.file_name}: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {document.file_name}: {e}")
                continue
        
        return extracted_texts
    
    async def _extract_text_from_pdf(
        self, 
        file_data: bytes, 
        file_name: str
    ) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        
        if not fitz:
            logger.error("PyMuPDF –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install PyMuPDF")
            return None
        
        try:
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º PDF –∏–∑ –±–∞–π—Ç–æ–≤
            pdf_document = fitz.open(stream=file_data, filetype="pdf")
            
            text_parts = []
            
            for page_num in range(pdf_document.page_count):
                page = pdf_document.load_page(page_num)
                text = page.get_text()
                text_parts.append(text)
            
            pdf_document.close()
            
            full_text = "\n".join(text_parts)
            
            # –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            full_text = self._clean_extracted_text(full_text)
            
            return full_text
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF {file_name}: {e}")
            return None
    
    def _clean_extracted_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
        import re
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {2,}', ' ', text)
        
        return text
    
    def _determine_bki_type(
        self, 
        document_type: DocumentType, 
        text: str
    ) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –ë–ö–ò –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        type_mapping = {
            DocumentType.CREDIT_REPORT_NBKI: "–ù–ë–ö–ò",
            DocumentType.CREDIT_REPORT_OKB: "–û–ö–ë", 
            DocumentType.CREDIT_REPORT_EQUIFAX: "–≠–∫–≤–∏—Ñ–∞–∫—Å"
        }
        
        if document_type in type_mapping:
            return type_mapping[document_type]
        
        # –ï—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–æ—Å—å - –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        text_lower = text.lower()
        
        if "–Ω–±–∫–∏" in text_lower or "–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω" in text_lower:
            return "–ù–ë–ö–ò"
        elif "–æ–∫–±" in text_lower or "–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω" in text_lower:
            return "–û–ö–ë"
        elif "—ç–∫–≤–∏—Ñ–∞–∫—Å" in text_lower or "equifax" in text_lower:
            return "–≠–∫–≤–∏—Ñ–∞–∫—Å"
        else:
            return "–ë–ö–ò_–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π"
    
    async def _combine_bki_texts(self, extracted_texts: Dict[str, str]) -> str:
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –ë–ö–ò –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        
        combined_parts = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        combined_parts.append("=== –û–ë–™–ï–î–ò–ù–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –ö–†–ï–î–ò–¢–ù–û–ô –ò–°–¢–û–†–ò–ò ===\n")
        
        # –ü–æ—Ä—è–¥–æ–∫ –ë–ö–ò –∫–∞–∫ –≤ –ø—Ä–æ—Ç–æ–∫–æ–ª–µ: –ù–ë–ö–ò, –û–ö–ë, –≠–∫–≤–∏—Ñ–∞–∫—Å
        bki_order = ["–ù–ë–ö–ò", "–û–ö–ë", "–≠–∫–≤–∏—Ñ–∞–∫—Å"]
        
        for bki_name in bki_order:
            if bki_name in extracted_texts:
                combined_parts.append(f"\n{'='*50}")
                combined_parts.append(f"–ë–õ–û–ö: –û–¢–ß–ï–¢ {bki_name}")
                combined_parts.append(f"{'='*50}\n")
                combined_parts.append(extracted_texts[bki_name])
                combined_parts.append(f"\n{'='*50}")
                combined_parts.append(f"–ö–û–ù–ï–¶ –ë–õ–û–ö–ê {bki_name}")
                combined_parts.append(f"{'='*50}\n")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ë–ö–ò –µ—Å–ª–∏ –µ—Å—Ç—å
        for bki_name, text in extracted_texts.items():
            if bki_name not in bki_order:
                combined_parts.append(f"\n{'='*50}")
                combined_parts.append(f"–ë–õ–û–ö: –û–¢–ß–ï–¢ {bki_name}")
                combined_parts.append(f"{'='*50}\n")
                combined_parts.append(text)
                combined_parts.append(f"\n{'='*50}")
                combined_parts.append(f"–ö–û–ù–ï–¶ –ë–õ–û–ö–ê {bki_name}")
                combined_parts.append(f"{'='*50}\n")
        
        combined_text = "\n".join(combined_parts)
        
        logger.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return combined_text
    
    async def _send_to_gpt(self, combined_text: str) -> Dict[str, Any]:
        """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –≤ GPT –Ω–∞ –∞–Ω–∞–ª–∏–∑"""
        
        if not openai or not openai.api_key:
            return {
                "success": False,
                "error": "OpenAI API –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω"
            }
        
        # –ß–∏—Ç–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = await self._get_analysis_prompt()
        
        try:
            logger.info("–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ GPT...")
            
            response = await openai.ChatCompletion.acreate(
                model="gpt-4",  # –∏–ª–∏ gpt-3.5-turbo –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏
                messages=[
                    {
                        "role": "system", 
                        "content": prompt
                    },
                    {
                        "role": "user", 
                        "content": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫—Ä–µ–¥–∏—Ç–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é:\n\n{combined_text}"
                    }
                ],
                max_tokens=4000,
                temperature=0.1  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            )
            
            gpt_response = response.choices[0].message.content
            
            logger.info(f"–ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç GPT: {len(gpt_response)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return {
                "success": True,
                "response": gpt_response,
                "tokens_used": response.usage.total_tokens
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ GPT: {e}")
            return {
                "success": False,
                "error": f"–û—à–∏–±–∫–∞ GPT API: {str(e)}"
            }
    
    async def _get_analysis_prompt(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –Ω–∞—à–µ–≥–æ —Ñ–∞–π–ª–∞
        # TODO: –ú–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –∏–ª–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        
        return """üß† –ü–û–î–†–û–ë–ù–´–ô –ü–†–û–ú–¢: –ö–ò-–ê–Ω–∞–ª–∏—Ç–∏–∫ (1-–π —ç—Ç–∞–ø ‚Äî –ø–æ–∏—Å–∫ –æ—à–∏–±–æ–∫ –∏ —Ä–∞—Å—á—ë—Ç—ã)
–¢—ã ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏. –†–∞–±–æ—Ç–∞–µ—à—å –ø–æ —Ç—Ä—ë–º –æ—Ç—á—ë—Ç–∞–º –ë–ö–ò (–ù–ë–ö–ò, –û–ö–ë, –≠–∫–≤–∏—Ñ–∞–∫—Å) –∏ –∞–Ω–∫–µ—Ç–µ –∫–ª–∏–µ–Ω—Ç–∞. –ù–∞ –≤—ã—Ö–æ–¥–µ —Ç—ã –¥–æ–ª–∂–µ–Ω:
–ù–∞–π—Ç–∏ –≤—Å–µ –æ—à–∏–±–∫–∏, –¥—É–±–ª–∏, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è;
–í—ã—è–≤–∏—Ç—å —Å—Ç–æ–ø-—Ñ–∞–∫—Ç–æ—Ä—ã –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞;
–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–µ–∫—É—â—É—é –∫—Ä–µ–¥–∏—Ç–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É (–ü–î–ù, –ø–ª–∞—Ç–µ–∂–∏, –ø—Ä–æ—Å—Ä–æ—á–∫–∏);
–í—ã–¥–∞—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É –±–ª–æ–∫—É —á—ë—Ç–∫–∏–π –æ—Ç—á—ë—Ç: —á—Ç–æ –Ω–∞–π–¥–µ–Ω–æ, –≥–¥–µ, –ø–æ—á–µ–º—É —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ.

üìã –û–ë–©–ò–ï –ü–†–ê–í–ò–õ–ê –î–õ–Ø GPT:
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –æ—Ç—á—ë—Ç—ã –∏ –∞–Ω–∫–µ—Ç—É –∫–ª–∏–µ–Ω—Ç–∞ –±–ª–æ—á–Ω–æ.
–†–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω –¥—Ä—É–≥–æ–º—É GPT (–ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç—É).
–ù–µ –¥–µ–ª–∞–π –≤—ã–≤–æ–¥–æ–≤ –∏ –Ω–µ –¥–∞–≤–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.
–ü–∏—à–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ. –ù–∏–∫–∞–∫–∏—Ö —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π.
–ï—Å–ª–∏ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ‚Äî –ø–∏—à–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö.
–ï—Å–ª–∏ –Ω–µ—Ç –æ—à–∏–±–æ–∫ ‚Äî –ø–∏—à–∏ –æ—à–∏–±–æ–∫ –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ.
–£–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫: –∏–∑ –∫–∞–∫–æ–≥–æ –ë–ö–ò, –ø–æ –∫–∞–∫–æ–º—É –¥–æ–≥–æ–≤–æ—Ä—É, —Å –∫–∞–∫–∏–º –Ω–æ–º–µ—Ä–æ–º –∏ –¥–∞—Ç–æ–π.

–ê–ù–ê–õ–ò–ó–ò–†–£–ô –ü–û –°–õ–ï–î–£–Æ–©–ò–ú 12 –ë–õ–û–ö–ê–ú:

–ë–ª–æ–∫ 1. –û—à–∏–±–∫–∏ –≤ —Ç–∏—Ç—É–ª–µ
–ë–ª–æ–∫ 2. –û—à–∏–±–∫–∏ –≤ —Ä–µ–∫–≤–∏–∑–∏—Ç–∞—Ö  
–ë–ª–æ–∫ 3. –ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
–ë–ª–æ–∫ 4. –ù–µ–∑–∞–∫—Ä—ã—Ç—ã–µ —Å—á–µ—Ç–∞
–ë–ª–æ–∫ 5. –ü–ª–æ—Ö–∏–µ —Å—á–µ—Ç–∞ (–ú–§–û, –ñ–ö–•, –∫–æ–ª–ª–µ–∫—Ç–æ—Ä—ã)
–ë–ª–æ–∫ 6. –†–∞–∑–Ω–æ—á—Ç–µ–Ω–∏—è –º–µ–∂–¥—É –ë–ö–ò
–ë–ª–æ–∫ 7. –û—à–∏–±–∫–∏ –≤ –ø–ª–∞—Ç—ë–∂–Ω–æ–π –¥–∏—Å—Ü–∏–ø–ª–∏–Ω–µ
–ë–ª–æ–∫ 8. –ó–∞–¥–≤–æ–µ–Ω–∏–µ —Å—á–µ—Ç–æ–≤
–ë–ª–æ–∫ 9. –ù–µ–æ–±–Ω—É–ª—ë–Ω–Ω—ã–µ —Å—á–µ—Ç–∞
–ë–ª–æ–∫ 10. –°—Ç–æ–ø-–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
–ë–ª–æ–∫ 11. –ù–µ–≤–µ—Ä–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤
–ë–ª–æ–∫ 12. –ù–µ–∑–∞–∫–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

–§–û–†–ú–ê–¢ –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê:
–ë–ª–æ–∫ X. –ù–∞–∑–≤–∞–Ω–∏–µ
–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å: üü•/üü®/üü©
[–û–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞]

–í –ö–û–ù–¶–ï –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û:
–°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞:
–í—Å–µ–≥–æ –±–ª–æ–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 12
–ë–ª–æ–∫–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: N
–ë–ª–æ–∫–æ–≤ –±–µ–∑ –æ—à–∏–±–æ–∫: M  
–ë–ª–æ–∫–æ–≤ —Å –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ–º –¥–∞–Ω–Ω—ã—Ö: K"""
    
    async def _parse_gpt_response(self, gpt_response: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ GPT"""
        
        # –ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å
        analysis_result = {
            "raw_response": gpt_response,
            "blocks": {},
            "summary": {},
            "parsed_at": datetime.utcnow().isoformat()
        }
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ –±–ª–æ–∫–æ–≤
        lines = gpt_response.split('\n')
        current_block = None
        current_content = []
        
        for line in lines:
            if line.startswith('–ë–ª–æ–∫') and '.' in line:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –±–ª–æ–∫
                if current_block and current_content:
                    analysis_result["blocks"][current_block] = '\n'.join(current_content)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –±–ª–æ–∫
                current_block = line.strip()
                current_content = [line]
                
            elif current_block:
                current_content.append(line)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–ª–æ–∫
        if current_block and current_content:
            analysis_result["blocks"][current_block] = '\n'.join(current_content)
        
        # –ò—â–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if "–°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞" in gpt_response:
            status_section = gpt_response.split("–°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞")[-1]
            analysis_result["summary"]["status_section"] = status_section.strip()
        
        return analysis_result
    
    async def _save_analysis_result(
        self, 
        user_id: int, 
        application_id: Optional[int], 
        analysis_result: Dict[str, Any]
    ):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞"""
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            if application_id:
                async with get_db_session() as session:
                    await session.execute(
                        update(Application)
                        .where(Application.id == application_id)
                        .values(
                            diagnosis_result=json.dumps(analysis_result, ensure_ascii=False),
                            updated_at=datetime.utcnow()
                        )
                    )
                    await session.commit()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            log_dir = "gpt_analysis_logs"
            os.makedirs(log_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = os.path.join(log_dir, f"analysis_{user_id}_{timestamp}.json")
            
            async with aiofiles.open(log_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(analysis_result, ensure_ascii=False, indent=2))
            
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {log_file}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")